<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="ico/favicon.ico">

    <title>SPAN: Understanding a Question with its Support Answers -- Liang Pang</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="css/custom.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>
    <header class="main">
        <div class="container">
            <div class="row">
                <div class="col-sm-12">
                    <h1>SPAN</h1>
                    <h2>Understanding a Question with its Support Answers</h2>
                </div>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="row">
            <h3> ABOUT ME </h3>
            <hr/>
            <div class="col-sm-6">
                <p><b>Liang Pang</b> - <b>庞亮</b></p>
                <p>PhD Student</p>
                <p>CAS Key Laboratory of Network Data Science and Technology</p>
                <p>Institute of Computing Technology, Chinese Academy of Sciences</p>
            </div>
            <div class="col-sm-6">
                <p> . </p>
                <p> . </p>
                <p>Email: pangliang {AT} software {DOT} ict.ac.cn</p>
                <p>GitHub: <a href="https://github.com/pl8787"> pl8787 </a> </p>
            </div>
        </div>
        <div class="row">
            <h3> PROBLEM </h3>
            <hr/>
            <div class="col-sm-12">
                <p>Community question answering (CQA) sites have become very popular in recent years [<b>surdeanu2008learning</b>]. Information seekers post their questions on the CQA website and other users can give some replies to the question. Therefore, it is valuable if we can automatically select the best answer from the candidate answers.
                </p>
            </div>
            <p> <b>For example:</b> </p>
            <div class="col-sm-4">
                <b><a href="https://answers.yahoo.com/"> Yahoo! Answers </a></b>
                <br/>
                <img src="images/qa/yahoo.png" height="400">
            </div>
            <div class="col-sm-4">
                <b><a href="http://zhidao.baidu.com/"> Baidu Zhidao </a></b>
                <br/>
                <img src="images/qa/baidu.png" height="400">
            </div>
            <div class="col-sm-4">
                <b><a href="https://www.quora.com/"> Quora </a></b>
                <br/>
                <img src="images/qa/quora.png" height="400">
            </div>
        </div>
        <div class="row">
            <h3> MOTIVATION </h3>
            <hr/>
            <div class="col-sm-12">
                <p>Recently, deep learning methods has been applied to this task and gain state-of-the-art performances. Most existing deep models [<b>yu2014deep,qiuconvolutional</b>] are directly using similarities between question and answer by their individual sentence embeddings, obtained by convolutional sentence model (CSM). Such deep models are effective in solving the mismatching problem, therefore they usually work well in distinguishing the best answers of one question with those of other questions. However, the information is usually limited in the descriptions of question, and there is always some lexical gap between question and answer in the application of CQA. These issues make the above deep learning approach far from solving the problem of selecting the best answer from the candidates with respect to one question.</p>

                <p>In this paper, we propose a novel deep architecture, namely SPAN, to tackle the above challenges. The main idea comes from the assumption that <i> similar questions usually have similar answers</i>. Based on the above assumption, we can better understand a question with its similar questions' best answers, defined as <b> support answers </b> in this paper. We can see that support answers provide additional content for a question. Furthermore, the lexical gap between question and answer is largely bridged by using support answers to represent a question. Firstly, a deep model is used to generate the sentence embeddings of the question, candidate answer, and the support answers. Then two similarities are computed, one is between question and the candidate answer, and the other one is between support answers and the candidate answer. Finally, the matching score is produced by combing them. Please note that SPAN is a general architecture where any kind of deep model can be used as the basic component to generate the sentence embedding. In this paper, we use CSM as an example to facilitate the study.</p>
            </div>
        </div>
        <div class="row">
            <h3> MODEL </h3>
            <hr/>
            <div class="col-sm-12">
                <p>
                    The input of CSM is a sentence <img src = "http://latex.codecogs.com/gif.latex?T" />, where each word <img src = "http://latex.codecogs.com/gif.latex?w_i" /> in <img src = "http://latex.codecogs.com/gif.latex?T" /> is represented as its word embedding initialed by Word2Vec. Then one dimensional convolution and pooling is then applied to the sentence layer by layer, and a sentence embedding will be generated as the output of CSM.
                </p>
                <img src="images/qa/models.svg" height="300" width="1000" />
                <p>
                    The architecture of SPAN is illustrated in Fig 1B. Suppose there is a question <img src = "http://latex.codecogs.com/gif.latex?Q" /> and a candidate answer <img src = "http://latex.codecogs.com/gif.latex?A" />, they are both feeded into a CSM to obtain their sentence embeddings, denoted as <img src = "http://latex.codecogs.com/gif.latex?v_Q" /> and <img src = "http://latex.codecogs.com/gif.latex?v_A" />, respectively. Simultaneously, we also leverage the support answers to help understand the semantic of the question. Specifically, we use BM25 [<b>robertson2009probabilistic</b>], a common retrieval model, to obtain the similar training questions of the original question. Then their best answers are extracted as the support answers, denoted as <img src = "http://latex.codecogs.com/gif.latex?SA_Q" />. They are also feeded into CSM to obtain the sentence embeddings <img src = "http://latex.codecogs.com/gif.latex?v_{SA}^{(i)},i=1,\dots,m" />. Based on those above sentence embeddings, we can obtain two kinds of similarities. The first one is between the question and candidate answer, and the second one is between the support answers and the candidate answer. The similarity measure can be any kind of operators, such as Cosine, Bilinear, and Tensor, denoted as <img src = "http://latex.codecogs.com/gif.latex?\otimes" />. The matching score is finally produced by combing these similarities, described as follows.
                </p>
                <p>
                    <img src = "http://latex.codecogs.com/gif.latex?S(Q,A)=\lambda_1v_Q\otimes v_A+ \sum_{i=1}^m\lambda_{2i}v_A\otimes v_{SA}^{(i)}" />
                    <br/>
                    where <img src = "http://latex.codecogs.com/gif.latex?\lambda_1" /> and <img src = "http://latex.codecogs.com/gif.latex?\lambda_{2i}" /> are combining parameters, which are tuned by hand on validation set.
                </p>
                <p>
                All other parameters, such as word embeddings and weights in convolution, are learned in the training process. Specifically, we use the ranking loss for optimization. Given a training question <img src = "http://latex.codecogs.com/gif.latex?Q" /> and its candidate answers <img src = "http://latex.codecogs.com/gif.latex?A_i,i=1,\dots,n" />, we denote <img src = "http://latex.codecogs.com/gif.latex?B_Q" /> as the best answer of <img src = "http://latex.codecogs.com/gif.latex?Q" />. Then we can construct <img src = "http://latex.codecogs.com/gif.latex?n-1" /> pairs, denoted as <img src = "http://latex.codecogs.com/gif.latex?(Q,B_Q,A_i)" />, where <img src = "http://latex.codecogs.com/gif.latex?A_i\neq B_Q" />. The loss function on each pair is defined as:
                <br/>
                <img src = "http://latex.codecogs.com/gif.latex?L(Q, B_Q, A_i)=\max \bigl(0, 1-S(Q,B_Q)+S(Q,A_i) \bigr)"/>
                </p>
                
            </div>
        </div>
        <div class="row">
            <h3> EXPERIMENTS </h3>
            <hr/>
            <div class="col-sm-12">
                <p>
                    We conduct experiments on <a href="http://webscope.sandbox.yahoo.com">Yahoo! Answers</a> dataset to evaluate our SPAN. The data set contains 142,627 questions and their candidate answers. We first filter out the questions which only contain one candidate answer or have less then three similar questions. The remaining 123,032 questions are then splitted into three set, training, validation, and testing set, which contains 98,426, 12,303, and 12,303 questions, respectively.
                </p>
                <p>
                    In testing set <img src = "http://latex.codecogs.com/gif.latex?\mathbb{Q}" />, a ranking list of the candidate answers are obtained according to the descending order of the matching scores. The evaluation metrics of our experiments are P@1 and MRR. Since each question only has one best answer, the two evaluation measures are of the following forms.
                </p>
                <img src = "http://latex.codecogs.com/gif.latex?\text{P@1}=\frac{1}{|\mathbb{Q}|} \sum_{Q \in \mathbb{Q}} \mathbb{I}(r_Q=1),\ \  \text{MRR} = \frac{1}{|\mathbb{Q}|} \sum_{Q \in \mathbb{Q}} \frac{1}{r_Q}" /> where <img src = "http://latex.codecogs.com/gif.latex?r_Q" /> denotes the rank of the best answer.
                <table class="table table-striped">
                    <thead>
                        <tr>
                          <th>Model</th>
                          <th>Random</th>
                          <th>BM25</th>
                          <th>CSM</th>
                          <th>SPAN</th>
                          <th>SPAN-SA</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <th scope="row">P@1</th>
                          <td>25.1</td>
                          <td>39.4</td>
                          <td>47.6</td>
                          <td><b>48.5</b></td>
                          <td>48.3</td>
                        </tr>
                        <tr>
                          <th scope="row">MRR</th>
                          <td>48.4</td>
                          <td>59.9</td>
                          <td>66.6</td>
                          <td><b>67.2</b></td>
                          <td>67.1</td>
                        </tr>
                      </tbody>
                </table>

                <p>
                    Three baselines are used in our model, i.e.Random, BM25, and CSM. Random indicate the model of directly selecting a random ranking list for evaluation. BM25 indicate the model of using BM25 to calculate the similarity between question and its candidate answers to obtain the ranking list. The parameters of BM25 are set to be <img src = "http://latex.codecogs.com/gif.latex?k1=0.3" /> and <img src = "http://latex.codecogs.com/gif.latex?b=0.05" />, which are tuned by grid search on validation set. For SPAN, <img src = "http://latex.codecogs.com/gif.latex?\lambda_1" /> and <img src = "http://latex.codecogs.com/gif.latex?\lambda_{2i}" /> are set to be equal in our experiments, with other parameters learned automatically. The experimental results are listed in Table 1.
                </p>
                <p>
                    From the results, we can see that SPAN outperforms the three baselines. This demonstrates that the introduction of support answers can largely alleviate the problem of infor- mation lack in question description and lexical gap between question and answer, and facilitate the matching process. We also list the results of SPAN by only using the representa- tions of support answers, denoted as SPAN-SA. The results show that it can also beat the three baselines, indicating that support answers themselves can be viewed as good repre- sentations for a question in this task.
                </p>
            </div>
        </div>
        <div class="row">
            <h3> DISCUSSION </h3>
            <hr/>
            <div class="col-sm-12">
                <p>
                    In this paper, we propose a novel deep architecture for CQA, namely SPAN. The main contribution is to introduce support answers to help understand the semantic of a question. Our experimental results show that the architecture performs better than several existing baselines. In future, we decide to test our idea on other more complex models such as Arc-II [<b>hu2014convolutional</b>] and LSTM [<b>palangi2015deep</b>]. We also want to investigate how to design an end-to-end model to automatically involve support answers in the learning process.
                </p>
            </div>
        </div>
        <div class="row">
            <h4> REFERENCE </h4>
            <hr/>
            <div class="col-sm-12">
                <dl class="dl-horizontal">
                  <dt>surdeanu2008learning</dt>
                  <dd>Surdeanu, M.; Ciaramita, M.; and Zaragoza, H. 2008. Learning to rank answers on large online qa collections. In ACL, 719– 727.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>hu2014convolutional</dt>
                    <dd>Hu, B.; Lu, Z.; Li, H.; and Chen, Q. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in NIPS, 2042–2050.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>palangi2015deep</dt>
                    <dd>Palangi, H.; Deng, L.; Shen, Y.; Gao, J.; He, X.; Chen, J.; Song, X.; and Ward, R. 2015. Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval. arXiv preprint arXiv:1502.06922.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>qiuconvolutional</dt>
                    <dd>Qiu, X., and Huang, X. Convolutional neural tensor network architecture for community-based question answering.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>robertson2009probabilistic</dt>
                    <dd>Robertson, S., and Zaragoza, H. 2009. The probabilistic rele- vance framework: BM25 and beyond. Now Publishers Inc.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>socher2011dynamic</dt>
                    <dd>Socher, R.; Huang, E. H.; Pennin, J.; Manning, C. D.; and Ng, A. Y. 2011. Dynamic pooling and unfolding recursive autoen- coders for paraphrase detection. In Advances in NIPS, 801–809.</dd>
                </dl>
                <dl class="dl-horizontal">
                    <dt>yu2014deep</dt>
                    <dd>Yu, L.; Hermann, K. M.; Blunsom, P.; and Pulman, S. 2014. Deep learning for answer sentence selection. arXiv preprint arXiv:1412.1632.</dd>
                </dl>
            </div>
        </div>
    </div>

    <!-- FOOTER -->
    <footer class='main'>
        <p>Edit by Liang Pang 2015</p>
    </footer>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/docs.min.js"></script>
  </body>
</html>
